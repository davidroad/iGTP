{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8f6415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys, os\n",
    "import requests\n",
    "from torch_geometric.data import Data\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from dcor import distance_correlation\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parse_single_pert(i):\n",
    "    a = i.split('+')[0]\n",
    "    b = i.split('+')[1]\n",
    "    if a == 'ctrl':\n",
    "        pert = b\n",
    "    else:\n",
    "        pert = a\n",
    "    return pert\n",
    "\n",
    "def parse_combo_pert(i):\n",
    "    return i.split('+')[0], i.split('+')[1]\n",
    "\n",
    "def combine_res(res_1, res_2):\n",
    "    res_out = {}\n",
    "    for key in res_1:\n",
    "        res_out[key] = np.concatenate([res_1[key], res_2[key]])\n",
    "    return res_out\n",
    "\n",
    "def parse_any_pert(p):\n",
    "    if ('ctrl' in p) and (p != 'ctrl'):\n",
    "        return [parse_single_pert(p)]\n",
    "    elif 'ctrl' not in p:\n",
    "        out = parse_combo_pert(p)\n",
    "        return [out[0], out[1]]\n",
    "\n",
    "def np_pearson_cor(x, y):\n",
    "    xv = x - x.mean(axis=0)\n",
    "    yv = y - y.mean(axis=0)\n",
    "    xvss = (xv * xv).sum(axis=0)\n",
    "    yvss = (yv * yv).sum(axis=0)\n",
    "    result = np.matmul(xv.transpose(), yv) / np.sqrt(np.outer(xvss, yvss))\n",
    "    # bound the values to -1 to 1 in the event of precision issues\n",
    "    return np.maximum(np.minimum(result, 1.0), -1.0)\n",
    "\n",
    "\n",
    "def dataverse_download(url, save_path):\n",
    "    \"\"\"\n",
    "    Dataverse download helper with progress bar\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        path (str): the path to save the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        print_sys(\"Downloading...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "\n",
    "        \n",
    "def zip_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for zip file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.zip')\n",
    "        print_sys('Extracting zip file...')\n",
    "        with ZipFile((save_path + '.zip'), 'r') as zip:\n",
    "            zip.extractall(path = data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def tar_data_download_wrapper(url, save_path, data_path):\n",
    "    \"\"\"\n",
    "    Wrapper for tar file download\n",
    "\n",
    "    Args:\n",
    "        url (str): the url of the dataset\n",
    "        save_path (str): the path where the file is donwloaded\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print_sys('Found local copy...')\n",
    "    else:\n",
    "        dataverse_download(url, save_path + '.tar.gz')\n",
    "        print_sys('Extracting tar file...')\n",
    "        with tarfile.open(save_path  + '.tar.gz') as tar:\n",
    "            tar.extractall(path= data_path)\n",
    "        print_sys(\"Done!\")  \n",
    "        \n",
    "def get_go_auto(gene_list, data_path, data_name):\n",
    "    \"\"\"\n",
    "    Get gene ontology data\n",
    "\n",
    "    Args:\n",
    "        gene_list (list): list of gene names\n",
    "        data_path (str): the path to save the extracted dataset\n",
    "        data_name (str): the name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        df_edge_list (pd.DataFrame): gene ontology edge list\n",
    "    \"\"\"\n",
    "    go_path = os.path.join(data_path, data_name, 'go.csv')\n",
    "    \n",
    "    if os.path.exists(go_path):\n",
    "        return pd.read_csv(go_path)\n",
    "    else:\n",
    "        ## download gene2go.pkl\n",
    "        if not os.path.exists(os.path.join(data_path, 'gene2go.pkl')):\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6153417'\n",
    "            dataverse_download(server_path, os.path.join(data_path, 'gene2go.pkl'))\n",
    "        with open(os.path.join(data_path, 'gene2go.pkl'), 'rb') as f:\n",
    "            gene2go = pickle.load(f)\n",
    "\n",
    "        gene2go = {i: list(gene2go[i]) for i in gene_list if i in gene2go}\n",
    "        edge_list = []\n",
    "        for g1 in tqdm(gene2go.keys()):\n",
    "            for g2 in gene2go.keys():\n",
    "                edge_list.append((g1, g2, len(np.intersect1d(gene2go[g1],\n",
    "                   gene2go[g2]))/len(np.union1d(gene2go[g1], gene2go[g2]))))\n",
    "\n",
    "        edge_list_filter = [i for i in edge_list if i[2] > 0]\n",
    "        further_filter = [i for i in edge_list if i[2] > 0.1]\n",
    "        df_edge_list = pd.DataFrame(further_filter).rename(columns = {0: 'gene1',\n",
    "                                                                      1: 'gene2',\n",
    "                                                                      2: 'score'})\n",
    "\n",
    "        df_edge_list = df_edge_list.rename(columns = {'gene1': 'source',\n",
    "                                                      'gene2': 'target',\n",
    "                                                      'score': 'importance'})\n",
    "        df_edge_list.to_csv(go_path, index = False)        \n",
    "        return df_edge_list\n",
    "\n",
    "class GeneSimNetwork():\n",
    "    \"\"\"\n",
    "    GeneSimNetwork class\n",
    "\n",
    "    Args:\n",
    "        edge_list (pd.DataFrame): edge list of the network\n",
    "        gene_list (list): list of gene names\n",
    "        node_map (dict): dictionary mapping gene names to node indices\n",
    "\n",
    "    Attributes:\n",
    "        edge_index (torch.Tensor): edge index of the network\n",
    "        edge_weight (torch.Tensor): edge weight of the network\n",
    "        G (nx.DiGraph): networkx graph object\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_list, gene_list, node_map):\n",
    "        \"\"\"\n",
    "        Initialize GeneSimNetwork class\n",
    "        \"\"\"\n",
    "\n",
    "        self.edge_list = edge_list\n",
    "        self.G = nx.from_pandas_edgelist(self.edge_list, source='source',\n",
    "                        target='target', edge_attr=['importance'],\n",
    "                        create_using=nx.DiGraph())    \n",
    "        self.gene_list = gene_list\n",
    "        for n in self.gene_list:\n",
    "            if n not in self.G.nodes():\n",
    "                self.G.add_node(n)\n",
    "        \n",
    "        edge_index_ = [(node_map[e[0]], node_map[e[1]]) for e in\n",
    "                      self.G.edges]\n",
    "        self.edge_index = torch.tensor(edge_index_, dtype=torch.long).T\n",
    "        #self.edge_weight = torch.Tensor(self.edge_list['importance'].values)\n",
    "        \n",
    "        edge_attr = nx.get_edge_attributes(self.G, 'importance') \n",
    "        importance = np.array([edge_attr[e] for e in self.G.edges])\n",
    "        self.edge_weight = torch.Tensor(importance)\n",
    "\n",
    "def get_GO_edge_list(args):\n",
    "    \"\"\"\n",
    "    Get gene ontology edge list\n",
    "    \"\"\"\n",
    "    g1, gene2go = args\n",
    "    edge_list = []\n",
    "    for g2 in gene2go.keys():\n",
    "        score = len(gene2go[g1].intersection(gene2go[g2])) / len(\n",
    "            gene2go[g1].union(gene2go[g2]))\n",
    "        if score > 0.1:\n",
    "            edge_list.append((g1, g2, score))\n",
    "    return edge_list\n",
    "        \n",
    "def make_GO(data_path, pert_list, data_name, num_workers=25, save=True):\n",
    "    \"\"\"\n",
    "    Creates Gene Ontology graph from a custom set of genes\n",
    "    \"\"\"\n",
    "\n",
    "    fname = './data/go_essential_' + data_name + '.csv'\n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "\n",
    "    with open(os.path.join(data_path, 'gene2go_all.pkl'), 'rb') as f:\n",
    "        gene2go = pickle.load(f)\n",
    "    gene2go = {i: gene2go[i] for i in pert_list}\n",
    "\n",
    "    print('Creating custom GO graph, this can take a few minutes')\n",
    "    with Pool(num_workers) as p:\n",
    "        all_edge_list = list(\n",
    "            tqdm(p.imap(get_GO_edge_list, ((g, gene2go) for g in gene2go.keys())),\n",
    "                      total=len(gene2go.keys())))\n",
    "    edge_list = []\n",
    "    for i in all_edge_list:\n",
    "        edge_list = edge_list + i\n",
    "\n",
    "    df_edge_list = pd.DataFrame(edge_list).rename(\n",
    "        columns={0: 'source', 1: 'target', 2: 'importance'})\n",
    "    \n",
    "    if save:\n",
    "        print('Saving edge_list to file')\n",
    "        df_edge_list.to_csv(fname, index=False)\n",
    "\n",
    "    return df_edge_list\n",
    "\n",
    "def get_similarity_network(network_type, adata, threshold, k,\n",
    "                           data_path, data_name, split, seed, train_gene_set_size,\n",
    "                           set2conditions, default_pert_graph=True, pert_list=None):\n",
    "    \n",
    "    if network_type == 'co-express':\n",
    "        df_out = get_coexpression_network_from_train(adata, threshold, k,\n",
    "                                                     data_path, data_name, split,\n",
    "                                                     seed, train_gene_set_size,\n",
    "                                                     set2conditions)\n",
    "    elif network_type == 'go':\n",
    "        if default_pert_graph:\n",
    "            server_path = 'https://dataverse.harvard.edu/api/access/datafile/6934319'\n",
    "            tar_data_download_wrapper(server_path, \n",
    "                                     os.path.join(data_path, 'go_essential_all'),\n",
    "                                     data_path)\n",
    "            df_jaccard = pd.read_csv(os.path.join(data_path, \n",
    "                                     'go_essential_all/go_essential_all.csv'))\n",
    "\n",
    "        else:\n",
    "            df_jaccard = make_GO(data_path, pert_list, data_name)\n",
    "\n",
    "        df_out = df_jaccard.groupby('target').apply(lambda x: x.nlargest(k + 1,\n",
    "                                    ['importance'])).reset_index(drop = True)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def get_coexpression_network_from_train(adata, threshold, k, data_path,\n",
    "                                        data_name, split, seed, train_gene_set_size,\n",
    "                                        set2conditions):\n",
    "    \"\"\"\n",
    "    Infer co-expression network from training data\n",
    "\n",
    "    Args:\n",
    "        adata (anndata.AnnData): anndata object\n",
    "        threshold (float): threshold for co-expression\n",
    "        k (int): number of edges to keep\n",
    "        data_path (str): path to data\n",
    "        data_name (str): name of dataset\n",
    "        split (str): split of dataset\n",
    "        seed (int): seed for random number generator\n",
    "        train_gene_set_size (int): size of training gene set\n",
    "        set2conditions (dict): dictionary of perturbations to conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    fname = os.path.join(os.path.join(data_path, data_name), split + '_'  +\n",
    "                         str(seed) + '_' + str(train_gene_set_size) + '_' +\n",
    "                         str(threshold) + '_' + str(k) +\n",
    "                         '_co_expression_network.csv')\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        return pd.read_csv(fname)\n",
    "    else:\n",
    "        gene_list = [f for f in adata.var.gene_name.values]\n",
    "        idx2gene = dict(zip(range(len(gene_list)), gene_list)) \n",
    "        X = adata.X\n",
    "        train_perts = set2conditions['train']\n",
    "        X_tr = X[np.isin(adata.obs.condition, [i for i in train_perts if 'ctrl' in i])]\n",
    "        gene_list = adata.var['gene_name'].values\n",
    "\n",
    "        X_tr = X_tr.toarray()\n",
    "        out = np_pearson_cor(X_tr, X_tr)\n",
    "        out[np.isnan(out)] = 0\n",
    "        out = np.abs(out)\n",
    "\n",
    "        out_sort_idx = np.argsort(out)[:, -(k + 1):]\n",
    "        out_sort_val = np.sort(out)[:, -(k + 1):]\n",
    "\n",
    "        df_g = []\n",
    "        for i in range(out_sort_idx.shape[0]):\n",
    "            target = idx2gene[i]\n",
    "            for j in range(out_sort_idx.shape[1]):\n",
    "                df_g.append((idx2gene[out_sort_idx[i, j]], target, out_sort_val[i, j]))\n",
    "\n",
    "        df_g = [i for i in df_g if i[2] > threshold]\n",
    "        df_co_expression = pd.DataFrame(df_g).rename(columns = {0: 'source',\n",
    "                                                                1: 'target',\n",
    "                                                                2: 'importance'})\n",
    "        df_co_expression.to_csv(fname, index = False)\n",
    "        return df_co_expression\n",
    "    \n",
    "def filter_pert_in_go(condition, pert_names):\n",
    "    \"\"\"\n",
    "    Filter perturbations in GO graph\n",
    "\n",
    "    Args:\n",
    "        condition (str): whether condition is 'ctrl' or not\n",
    "        pert_names (list): list of perturbations\n",
    "    \"\"\"\n",
    "\n",
    "    if condition == 'ctrl':\n",
    "        return True\n",
    "    else:\n",
    "        cond1 = condition.split('+')[0]\n",
    "        cond2 = condition.split('+')[1]\n",
    "        num_ctrl = (cond1 == 'ctrl') + (cond2 == 'ctrl')\n",
    "        num_in_perts = (cond1 in pert_names) + (cond2 in pert_names)\n",
    "        if num_ctrl + num_in_perts == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def uncertainty_loss_fct(pred, logvar, y, perts, reg = 0.1, ctrl = None,\n",
    "                         direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Uncertainty loss function\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        logvar (torch.tensor): log variance\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        reg (float): regularization parameter\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2                     \n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "    for p in set(perts):\n",
    "        if p!= 'ctrl':\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[np.where(perts==p)[0]][:, retain_idx]\n",
    "            y_p = y[np.where(perts==p)[0]][:, retain_idx]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[np.where(perts==p)[0]]\n",
    "            y_p = y[np.where(perts==p)[0]]\n",
    "            logvar_p = logvar[np.where(perts==p)[0]]\n",
    "                         \n",
    "        # uncertainty based loss\n",
    "        losses += torch.sum((pred_p - y_p)**(2 + gamma) + reg * torch.exp(\n",
    "            -logvar_p)  * (pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        # direction loss                 \n",
    "        if p!= 'ctrl':\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses += torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl) -\n",
    "                                 torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "            \n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "\n",
    "def loss_fct(pred, y, perts, ctrl = None, direction_lambda = 1e-3, dict_filter = None):\n",
    "    \"\"\"\n",
    "    Main MSE Loss function, includes direction loss\n",
    "\n",
    "    Args:\n",
    "        pred (torch.tensor): predicted values\n",
    "        y (torch.tensor): true values\n",
    "        perts (list): list of perturbations\n",
    "        ctrl (str): control perturbation\n",
    "        direction_lambda (float): direction loss weight hyperparameter\n",
    "        dict_filter (dict): dictionary of perturbations to conditions\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = 2\n",
    "    mse_p = torch.nn.MSELoss()\n",
    "    perts = np.array(perts)\n",
    "    losses = torch.tensor(0.0, requires_grad=True).to(pred.device)\n",
    "\n",
    "    for p in set(perts):\n",
    "        pert_idx = np.where(perts == p)[0]\n",
    "        \n",
    "        # during training, we remove the all zero genes into calculation of loss.\n",
    "        # this gives a cleaner direction loss. empirically, the performance stays the same.\n",
    "        if p!= 'ctrl':\n",
    "            retain_idx = dict_filter[p]\n",
    "            pred_p = pred[pert_idx][:, retain_idx]\n",
    "            y_p = y[pert_idx][:, retain_idx]\n",
    "        else:\n",
    "            pred_p = pred[pert_idx]\n",
    "            y_p = y[pert_idx]\n",
    "        losses = losses + torch.sum((pred_p - y_p)**(2 + gamma))/pred_p.shape[0]/pred_p.shape[1]\n",
    "                         \n",
    "        ## direction loss\n",
    "        if (p!= 'ctrl'):\n",
    "            losses = losses + torch.sum(direction_lambda *\n",
    "                                (torch.sign(y_p - ctrl[retain_idx]) -\n",
    "                                 torch.sign(pred_p - ctrl[retain_idx]))**2)/\\\n",
    "                                 pred_p.shape[0]/pred_p.shape[1]\n",
    "        else:\n",
    "            losses = losses + torch.sum(direction_lambda * (torch.sign(y_p - ctrl) -\n",
    "                                                torch.sign(pred_p - ctrl))**2)/\\\n",
    "                                                pred_p.shape[0]/pred_p.shape[1]\n",
    "    return losses/(len(set(perts)))\n",
    "\n",
    "\n",
    "def print_sys(s):\n",
    "    \"\"\"system print\n",
    "\n",
    "    Args:\n",
    "        s (str): the string to print\n",
    "    \"\"\"\n",
    "    print(s, flush = True, file = sys.stderr)\n",
    "    \n",
    "def create_cell_graph_for_prediction(X, pert_idx, pert_gene):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph for inference\n",
    "\n",
    "    Args:\n",
    "        X (np.array): gene expression matrix\n",
    "        pert_idx (list): list of perturbation indices\n",
    "        pert_gene (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if pert_idx is None:\n",
    "        pert_idx = [-1]\n",
    "    return Data(x=torch.Tensor(X).T, pert_idx = pert_idx, pert=pert_gene)\n",
    "    \n",
    "\n",
    "def create_cell_graph_dataset_for_prediction(pert_gene, ctrl_adata, gene_names,\n",
    "                                             device, num_samples = 300):\n",
    "    \"\"\"\n",
    "    Create a perturbation specific cell graph dataset for inference\n",
    "\n",
    "    Args:\n",
    "        pert_gene (list): list of perturbations\n",
    "        ctrl_adata (anndata): control anndata\n",
    "        gene_names (list): list of gene names\n",
    "        device (torch.device): device to use\n",
    "        num_samples (int): number of samples to use for inference (default: 300)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices (and signs) of applied perturbation\n",
    "    pert_idx = [np.where(p == np.array(gene_names))[0][0] for p in pert_gene]\n",
    "\n",
    "    Xs = ctrl_adata[np.random.randint(0, len(ctrl_adata), num_samples), :].X.toarray()\n",
    "    # Create cell graphs\n",
    "    cell_graphs = [create_cell_graph_for_prediction(X, pert_idx, pert_gene).to(device) for X in Xs]\n",
    "    return cell_graphs\n",
    "\n",
    "##\n",
    "##GI related utils\n",
    "##\n",
    "\n",
    "def get_coeffs(singles_expr, first_expr, second_expr, double_expr):\n",
    "    \"\"\"\n",
    "    Get coefficients for GI calculation\n",
    "\n",
    "    Args:\n",
    "        singles_expr (np.array): single perturbation expression\n",
    "        first_expr (np.array): first perturbation expression\n",
    "        second_expr (np.array): second perturbation expression\n",
    "        double_expr (np.array): double perturbation expression\n",
    "\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results['ts'] = TheilSenRegressor(fit_intercept=False,\n",
    "                          max_subpopulation=1e5,\n",
    "                          max_iter=1000,\n",
    "                          random_state=1000)   \n",
    "    X = singles_expr\n",
    "    y = double_expr\n",
    "    results['ts'].fit(X, y.ravel())\n",
    "    Zts = results['ts'].predict(X)\n",
    "    results['c1'] = results['ts'].coef_[0]\n",
    "    results['c2'] = results['ts'].coef_[1]\n",
    "    results['mag'] = np.sqrt((results['c1']**2 + results['c2']**2))\n",
    "    \n",
    "    results['dcor'] = distance_correlation(singles_expr, double_expr)\n",
    "    results['dcor_singles'] = distance_correlation(first_expr, second_expr)\n",
    "    results['dcor_first'] = distance_correlation(first_expr, double_expr)\n",
    "    results['dcor_second'] = distance_correlation(second_expr, double_expr)\n",
    "    results['corr_fit'] = np.corrcoef(Zts.flatten(), double_expr.flatten())[0,1]\n",
    "    results['dominance'] = np.abs(np.log10(results['c1']/results['c2']))\n",
    "    results['eq_contr'] = np.min([results['dcor_first'], results['dcor_second']])/\\\n",
    "                        np.max([results['dcor_first'], results['dcor_second']])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_GI_params(preds, combo):\n",
    "    \"\"\"\n",
    "    Get GI parameters\n",
    "\n",
    "    Args:\n",
    "        preds (dict): dictionary of predictions\n",
    "        combo (list): list of perturbations\n",
    "\n",
    "    \"\"\"\n",
    "    singles_expr = np.array([preds[combo[0]], preds[combo[1]]]).T\n",
    "    first_expr = np.array(preds[combo[0]]).T\n",
    "    second_expr = np.array(preds[combo[1]]).T\n",
    "    double_expr = np.array(preds[combo[0]+'_'+combo[1]]).T\n",
    "    \n",
    "    return get_coeffs(singles_expr, first_expr, second_expr, double_expr)\n",
    "\n",
    "def get_GI_genes_idx(adata, GI_gene_file):\n",
    "    \"\"\"\n",
    "    Optional: Reads a file containing a list of GI genes (usually those\n",
    "    with high mean expression)\n",
    "\n",
    "    Args:\n",
    "        adata (anndata): anndata object\n",
    "        GI_gene_file (str): file containing GI genes (generally corresponds\n",
    "        to genes with high mean expression)\n",
    "    \"\"\"\n",
    "    # Genes used for linear model fitting\n",
    "    GI_genes = np.load(GI_gene_file, allow_pickle=True)\n",
    "    GI_genes_idx = np.where([g in GI_genes for g in adata.var.gene_name.values])[0]\n",
    "    \n",
    "    return GI_genes_idx\n",
    "\n",
    "def get_mean_control(adata):\n",
    "    \"\"\"\n",
    "    Get mean control expression\n",
    "    \"\"\"\n",
    "    mean_ctrl_exp = adata[adata.obs['condition'] == 'ctrl'].to_df().mean()\n",
    "    return mean_ctrl_exp\n",
    "\n",
    "def get_genes_from_perts(perts):\n",
    "    \"\"\"\n",
    "    Returns list of genes involved in a given perturbation list\n",
    "    \"\"\"\n",
    "\n",
    "    if type(perts) is str:\n",
    "        perts = [perts]\n",
    "    gene_list = [p.split('+') for p in np.unique(perts)]\n",
    "    gene_list = [item for sublist in gene_list for item in sublist]\n",
    "    gene_list = [g for g in gene_list if g != 'ctrl']\n",
    "    return list(np.unique(gene_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
